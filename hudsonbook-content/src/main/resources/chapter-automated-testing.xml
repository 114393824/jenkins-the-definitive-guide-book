<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="chapter-automated-testing">
  <title>Automated testing</title>

  <sect1>
    <title>Introduction</title>

    <para><indexterm>
        <primary>Automated testing</primary>
      </indexterm>Continuous Integration without automated testing isn't
    really Continuous Integration - it's simply a small improvement on
    automatically scheduled builds. Indeed, if you are using Hudson without
    any automated tests, you are not getting anywhere near as much value out
    of your Continuous Integration infrastructure as you should.</para>

    <para>One of the fundamental tenants of Continuous Integration is that a
    build should be verifiable. It must be possible to objectively determine
    whether a particular build artifact is ready to proceed to the next stage
    of the build process, and the most convenient way to do this is by using
    automated tests. Without proper automated testing set up in your build
    process, you will find yourself having to retain many build artifacts and
    test them by hand, which is hardly in the spirit of Continuous
    Integration.</para>

    <para>There are many ways you can integrate automated tests into your
    application. One of the most efficient ways to write high quality tests is
    to write them first, using techniques such as Test Driven Development
    (TDD) or Behaviour Driven Development (BDD). In this approach, commonly
    used in many Agile projects, the aim of your unit tests to both clarify
    your understanding of the code's behaviour and to write an automated test
    that the code does indeed implement this behaviour. Focusing on testing
    the expected behaviour, rather than the implementation, of your code also
    makes for more comprehensive and more accurate tests, and thus helps
    Hudson to provide more relevant feedback.</para>

    <para>Of course, more classical unit testing, done once the code has been
    implemented, is also another commonly-used approach.</para>

    <para>Hudson is not limited to unit testing, though. There are many other
    types of automated testing that you should consider, depending on the
    nature of your application, including integration testing, web testing,
    functional testing, performance testing, load testing and so on.</para>

    <para>Hudson can also be used, in conjunction with techniques like
    Behaviour-Driven Development and Acceptance Test Driven Development, as a
    communications tool aimed at both developers and other project
    stakeholders. BDD frameworks such as easyb, fitnesse, jbehave, rspec,
    Cucumber, and many others, try to present acceptance tests in terms that
    testers, product owners and end users can understand. With the use of such
    tools, Hudson can report on project progress in a non-developer friendly
    manner, and thus facilitate communication between developers and
    non-developers within a team.</para>

    <para>For existing or legacy applications with little or no automated
    testing in place, it can be time-consuming and difficult to retro-fit
    comprehensive unit tests onto the code. One useful approach in these
    situations is to write automated functional tests ("regression") tests
    that simulate the most common ways that users manipulate the application.
    For example, automated web testing tools such as Selenium and Web Driver
    can be used to web applications. While this approach is not as
    comprehensive as a combination of good quality unit, integration and
    acceptance tests, it is still an effective and relatively cost-efficient
    way to integrate automated regression testing into an existing
    application.</para>

    <para>In this chapter, we will see how Hudson helps you keep track of
    automated test results, and how you can use this information to monitor
    and dissect your build process.</para>
  </sect1>

  <sect1>
    <title>Automating your unit tests</title>

    <para>The first thing we will look at is how to integrate your unit tests
    into Hudson. Whether you are practicing Test-Driven Development, or
    writing unit tests using a more conventional approach, these are probably
    the first tests that you will want to automate with Hudson.</para>

    <para>Hudson does an excellent job of reporting on your test results.
    However, it is up to you to write the appropriate tests and to configure
    your build to run them automatically. Fortunately integrating unit tests
    into your automated builds is generally relatively easy.</para>

    <para>There are many unit testing tools out there. In the Java world,
    JUnit is the de facto standard in this area. TestNG is another popular
    Java unit testing framework. For C# applications, the NUnit testing
    framework proposes similar functionalities to those provided by JUnit, as
    does <command>Test::Unit</command> for Ruby.</para>

    <para>Hudson does an excellent job of reporting on your test results.
    However, it is up to you to write the appropriate tests and to configure
    your build to run them automatically. Fortunately integrating unit tests
    into your automated builds is relatively easy. In the rest of this
    section, we will look at how to do so with some of the more common build
    tools in use.</para>

    <sect2>
      <title>Automating your tests with Maven</title>

      <para>Maven is a popular open source build tool of the Java world, that
      makes use of practices such as declarative dependencies, standard
      directories and build life cycles, and convention over configuration to
      encourage clean, maintainable, high level build scripts. Test automation
      is strongly supported in Maven. Maven projects use a standard directory
      structure: it will automatically look for unit tests in a directory
      called <filename>src/test/java</filename>. There is little else to
      configure: just add a dependency to the test framework (or frameworks)
      your tests are using, and Maven will automatically look for and execute
      the JUnit, TestNG or even POJO (Plain Old Java Objects) tests contained
      in this directory structure.</para>

      <para>In Maven, you run your unit tests by invoking the
      <command>test</command> life cycle phase, as shown here:</para>

      <screen>$ <command>mvn test</command>
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Building Tweeter domain model
[INFO]    task-segment: [test]
[INFO] ------------------------------------------------------------------------
...
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.wakaleo.training.tweeter.domain.TagTest
Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.093 sec
Running com.wakaleo.training.tweeter.domain.TweeterTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 sec
Running com.wakaleo.training.tweeter.domain.TweeterUserTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.055 sec
Running com.wakaleo.training.tweeter.domain.TweetFeeRangeTest
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.051 sec
Running com.wakaleo.training.tweeter.domain.HamcrestTest
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.023 sec

Results :

Tests run: 38, Failures: 0, Errors: 0, Skipped: 0</screen>

      <para>In addition to executing your tests, and failing the build if any
      of the tests fail, Maven will produce a set of test reports in the
      <filename>target/surefire-reports</filename> directory, in both XML and
      text formats. For our CI purposes, it is the XML files that interest us,
      as Hudson is able to understand and analyse these files for it's CI
      reporting:</para>

      <screen>$ <command>ls target/surefire-reports/*.xml</command>
target/surefire-reports/TEST-com.wakaleo.training.tweeter.domain.HamcrestTest.xml
target/surefire-reports/TEST-com.wakaleo.training.tweeter.domain.TagTest.xml
target/surefire-reports/TEST-com.wakaleo.training.tweeter.domain.TweetFeeRangeTest.xml
target/surefire-reports/TEST-com.wakaleo.training.tweeter.domain.TweeterTest.xml
target/surefire-reports/TEST-com.wakaleo.training.tweeter.domain.TweeterUserTest.xml</screen>

      <para>Maven defines two distinct testing phases: unit tests and
      integration tests. Unit tests should be fast and lightweight, providing
      a large amount of test feedback in as little time as possible.
      Integration tests are slower and more cumbersome, and often require the
      application to be build and deployed to a server (even an embedded one)
      to carry out more complete tests. Both these sorts of tests are
      important, and for a well-designed Continuous Integration environment,
      it is important to be able to distinguish between them. The build should
      ensure that all of the unit tests are run initially - if a unit test
      fails, developers should be notified very quickly. Only if all of the
      unit tests pass is it worthwhile undertaking the slower and more
      heavy-weight integration tests.</para>

      <para>In Maven, integration tests are executed during the
      <command>integration-test</command> life cycle phase, which you can
      invoke by running '<command>mvn integration-test</command>' or (more
      simply) '<command>mvn verify</command>'. During this phase, it is easy
      to configure Maven to start up your web application on an embedded Jetty
      web server, or to package and deploy your application to a test server,
      for example. Your integration tests can then be executed against the
      running application. The tricky part however is telling Maven how to
      distinguish between your unit tests and your integration tests, so that
      they will only be executed when a running version of the application is
      available.</para>

      <para>There are several ways to do this, but at the time of writing
      there is no official standard approach used across all Maven projects.
      One simple strategy is to use naming conventions: all integration tests
      might end in 'IntegrationTest', or be placed in a particular package.
      The following class uses one such convention:</para>

      <para><programlisting>public class AccountIntegrationTest {
  
  @Test
  public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
  }
}</programlisting>In Maven, tests are configured via the
      <command>maven-surefire-plugin</command> plugin. To ensure that Maven
      only runs these tests during the <command>integration-test</command>
      phase, you can configure this plugin as shown here:</para>

      <programlisting>&lt;project&gt;
  ...
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;skip&gt;true&lt;/skip&gt;
        &lt;/configuration&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;unit-tests&lt;/id&gt;
            &lt;phase&gt;test&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;skip&gt;false&lt;/skip&gt;
              &lt;excludes&gt;
                &lt;exclude&gt;**/*IntegrationTest.java&lt;/exclude&gt;
              &lt;/excludes&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
          &lt;execution&gt;
            &lt;id&gt;integration-tests&lt;/id&gt;
            &lt;phase&gt;integration-test&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;skip&gt;false&lt;/skip&gt;
              &lt;includes&gt;
                &lt;include&gt;**/*IntegrationTest.java&lt;/include&gt;
              &lt;/includes&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
      ...</programlisting>

      <para>This will ensure that the integration tests are skipped during the
      unit test phase, and only executed during the integration test
      phase.</para>

      <para>If you are using TestNG, you can identify your integration tests
      using TestNG Groups. In TestNG, test classes or test methods can be
      tagged using the 'groups' attribute of the <command>@Test</command>
      annotation, as shown here:<programlisting>@Test(groups = { "integration-test" })
public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
}</programlisting></para>

      <para>Using Maven, you could ensure that these tests where only run
      during the integration test phase using the following
      configuration:</para>

      <para><programlisting>&lt;project&gt;
  ...
  &lt;build&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
        &lt;configuration&gt;
          &lt;skip&gt;true&lt;/skip&gt;
        &lt;/configuration&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;id&gt;unit-tests&lt;/id&gt;
            &lt;phase&gt;test&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;skip&gt;false&lt;/skip&gt;
              &lt;excludedGroups&gt;integration-tests&lt;/excludedGroups&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
          &lt;execution&gt;
            &lt;id&gt;integration-tests&lt;/id&gt;
            &lt;phase&gt;integration-test&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;test&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;skip&gt;false&lt;/skip&gt;
              &lt;groups&gt;integration-tests&lt;/groups&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
      ...</programlisting></para>
    </sect2>

    <sect2>
      <title>Automating your unit tests with Ant</title>

      <para>TODO</para>
    </sect2>

    <sect2>
      <title>Automating your unit tests with Gradle</title>

      <para>TODO</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Configuring JUnit reports in Hudson</title>

    <para>Once your build generates test results, you need to configure your
    Hudson build job to display them. For Maven build jobs, no special
    configuration is required - just make sure you invoke a goal that will run
    your tests, such as <command>mvn test</command> (for your unit tests) or
    <command>mvn verify</command> (for unit and integration tests), as shown
    in <xref linkend="fig-testing-maven-verify-goal" />.</para>

    <para><figure id="fig-testing-maven-verify-goal">
        <title>You configure your Hudson installation in the 'Manage Hudson'
        screen</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-maven-verify-goal.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>For free-style build jobs, you need to do a little more
    configuration work. In addition to ensuring that your build actually runs
    the tests, you need to tell Hudson to publish the JUnit test results
    report. You configure this in the <command>Post-build Actions</command>
    section (see <xref linkend="fig-testing-freestyle-junit-config" />). Here,
    you provide a path to the JUnit XML reports. Their exact location will
    depend on a project - for a Maven project, a path like
    '<filename>**/target/surefire-reports/*.xml</filename>' will find them for
    most projects. For an Ant-based project, it will depend on how you
    configured the Ant JUnit task, as we discussed above.</para>

    <para><figure id="fig-testing-freestyle-junit-config">
        <title>Configuring Maven test reports in a freestyle project</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-freestyle-junit-config.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure></para>
  </sect1>

  <sect1>
    <title>Displaying test results</title>

    <para>Once Hudson knows where to find the JUnit reports, it does an
    excellent job of reporting on them. Indeed, in many ways, Hudson's primary
    job is to detect and to report on build failures. And a failing unit test
    is one of the most obvious symptoms of a build failure.</para>

    <para>Hudson makes the distinction between <emphasis>failed</emphasis>
    builds and <emphasis>unstable</emphasis> builds. A failed build (indicated
    by a red ball) indicates test failures, or a build job that is broken in
    some brutal manner, such as a compilation error. An unstable build, on the
    other hand, is a build that is not considered of sufficient quality. What
    defines "quality" in this sense is largely up to you, but it is typically
    related to code quality metrics such as code coverage or coding standards,
    that we will be discussing later on in the book. For now, we will focus on
    the <emphasis>failed</emphasis> builds</para>

    <para>In <xref linkend="fig-testing-maven-test-failure-dashboard" /> we
    can see how Hudson displays a Maven build job containing test failures.
    The project home page is your first port of call when a build breaks. When
    a build results in failing tests, the 'Latest Test Result' link will
    indicate the current number of test failures in this build job ("5
    failures" in the illustration), and also the change in the number of test
    failures since the last build ("+5" in the illustration - five new test
    failures). Test failures will also appear as red in the 'Test Result
    Trend' graph to the right of the page.</para>

    <para><figure id="fig-testing-maven-test-failure-dashboard">
        <title>Hudson displays test result trends on the project home
        page</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-maven-test-failure-dashboard.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure>If you click on the 'Latest Test Result link, Hudson will
    display a summary of the current test results (see <xref
    linkend="fig-testing-test-result-details" />). For a Maven build job,
    Hudson will initially display a summary view of test results per module -
    to see the full details of the failing tests, just click on the module you
    are interest in. For a freestyle build job, Hudson will display the full
    failing tests view immediately. In both cases, Hudson starts off by
    presenting a summary of test results for each package. From here, you can
    drill down, seeing test results for each test class and then finally the
    tests within the test classes themselves. And if there are any failed
    tests, these will be prominently displayed at the top of the page.</para>

    <para><figure id="fig-testing-test-result-details">
        <title>Hudson displays a summary of the test results</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-test-result-details.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure>This full view gives you both a good overview of the current
    state of your tests, and an indication of their history. Indeed, the 'Age'
    column tells you how for how long a test has been broken, with a hyperlink
    that takes you back to the first build in which this test failed.</para>

    <para>You can also add a description to the test results, using the 'Edit
    Description' link in the top right-hand corner of the screen. This is a
    great way to annotate a build failure with some additional details, in
    order to add extra information about the origin of test failures for
    example.</para>

    <para>When a test fails, you generally want to know why. To see the
    details of a particular test failure, just click on the corresponding link
    on this screen. This will display all the gruesome details, including the
    error message and the stack trace, as well as a reminder of for how long
    the test has failing (see <xref
    linkend="fig-testing-test-failure-details" />). You should be wary of
    tests that have been failing for more than just a couple of builds - this
    is an indicator of either a tricky technical problem that might need
    investigating, or a complacent attitude to failed builds (developers might
    just be ignoring build failures), which is more serious and definitely
    should be investigated.</para>

    <para><figure id="fig-testing-test-failure-details">
        <title>The details of a test failure</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-test-failure-details.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure>It can also come in handy to keep tabs on how long your tests
    take to run, not just whether they pass or fail. Unit tests should be
    designed to run fast, and overly long-running tests can be the sign of a
    performance issue. Slow unit tests also delay feedback, and in CI, fast
    feedback is the name of the game. For example, running one thousand unit
    tests in five minutes is good - taking an hour to run them is not. So it
    is a good idea to regularly check how long your unit tests are taking to
    run, and if necessary investigate why they are taking so long.</para>

    <para>Luckily, Hudson can easily tell you how long your tests have been
    taking to run over time. On the build job home page, click on the "trend"
    link in the <command>Build History</command> box on the left of the
    screen. This will give you a graph along the lines of the one in <xref
    linkend="fig-testing-test-trend" />, showing how long each of your builds
    took to run. Now tests are not the only thing that happens in a build job,
    but if you have enough tests to worry about, they will probably take a
    large proportion of the time. So this graph is a great way to see how well
    your tests are performing as well.</para>

    <para><figure id="fig-testing-test-trend">
        <title>Build time trends can give you a good indicator of how fast
        your tests are running</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-test-trend.png" width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure>When you are on the Test Results page (see <xref
    linkend="fig-testing-test-result-details" />), you can also drill down and
    see how long the tests in a particular module, package or class are taking
    to run. Just click on the test duration in the test results page ("Took 31
    ms" in <xref linkend="fig-testing-test-result-details" />) to view the
    test history for a package, class, or individual test (see <xref
    linkend="fig-testing-test-result-history" />). This makes it easy to
    isolate a test that is taking more time then it should, or even decide
    when a general optimization of your unit tests is required.</para>

    <para><figure id="fig-testing-test-result-history">
        <title>Hudson also lets you see how long your tests take to
        run</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-test-result-history.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure></para>
  </sect1>

  <sect1>
    <title>Ignoring Tests</title>

    <para>Hudson distinguishes between test failures and skipped tests.
    Skipped tests are ones that have been deactivated, for example by using
    the <command>@Ignore</command> annotation in JUnit:</para>

    <para><programlisting>@Ignore("Pending more details from the BA")
@Test 
public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
}</programlisting></para>

    <para>Skipping tests is legitimate in some circumstances, such as to place
    an automated acceptance test, or higher-level technical test, on hold
    while you implement the lower levels. In such cases, you don't want to be
    distracted by the failing acceptance test, but you don't want to forget
    that the test exists either. Using techniques such as the
    <command>@Ignore</command> annotation are better than simply commenting
    out the test or renaming it (in JUnit 3), as it lets Hudson keep tabs on
    the ignored tests for you.</para>

    <para>In TestNG, you can also skip tests, using the 'enabled'
    property:</para>

    <para><programlisting>@Test(enabled=false)
public void cashWithdrawalShouldDeductSumFromBalance() throws Exception {
    Account account = new Account();
    account.makeDeposit(100);
    account.makeCashWithdraw(60);
    assertThat(account.getBalance(), is(40));
}</programlisting>In TestNG, you can also define dependencies between tests,
    so that certain tests will only run after another test or group of tests
    has run, as illustrated here:</para>

    <para><programlisting>@Test
public void serverStartedOk() {...}
 
@Test(dependsOnMethods = { "serverStartedOk" })
public void whenAUserLogsOnWithACorrectUsernameAndPasswordTheHomePageIsDisplayed() {...}</programlisting>In
    this case, if the first test (serverStartedOk()) fails, the following test
    will be skipped.</para>

    <para>In all of these cases, Hudson will mark the tests that were not run
    as yellow, both in the overall test results trend, and in the test details
    (see <xref linkend="fig-testing-test-skipped" />). Skipped tests are not
    as bad as test failures, but it is important not to get into the habit of
    neglecting them. Skipped tests are like branches in a version control
    system: a test should be skipped for a specific reason, with a clear idea
    as to when they will be reactivated. A skipped test that remains skipped
    for too long is a bad smell.</para>

    <para><figure id="fig-testing-test-skipped">
        <title>Hudson displays skipped tests as yellow</title>

        <mediaobject>
          <imageobject role="web" security="">
            <imagedata align="center"
                       fileref="figs/web/testing-test-skipped.png"
                       width="4.3in" />
          </imageobject>
        </mediaobject>
      </figure></para>
  </sect1>

  <sect1>
    <title>Code Coverage</title>

    <para>Another very useful test-related metric is code coverage. Code
    coverage give an indication of what parts of your application where
    executed during the tests. While this in itself is not a sufficient
    indication of quality testing (it is easy to execute an entire application
    without actually testing anything, and code coverage metrics provide no
    indication of the quality or accuracy of your tests), it is a very good
    indication of code that has <emphasis>not</emphasis> been tested. And, if
    your team is introducing rigorous testing practices such as
    Test-Driven-Development, code coverage can be a good indicator of how well
    these practices are being applied.</para>

    <para>Code coverage analysis is a CPU and memory-intensive process, and
    will slow down your build job significantly. For this reason, you will
    typically run code coverage metrics in a separate Hudson build job, to be
    run after your unit and integration tests are successful.</para>

    <para>In this section, we will look at two code coverage tools, and how to
    integrated them with Hudson: Cobertura, an open source code coverage tool,
    and Clover, a commercial code coverage tool from Atlassian.</para>

    <sect2>
      <title>Measuring code coverage with Cobertura</title>

      <para>Cobertura (<ulink url="http://cobertura.sourceforge.net"></ulink>)
      is an open source code coverage tool that is easy to use and integrates
      well with both Maven and Hudson.</para>

      <para>Like almost all of the Hudson code quality metrics
      plugins<footnote>
          <para>With the notable exception of Sonar, which we will look at
          later on in the book.</para>
        </footnote>, the Cobertura plugin for Hudson will not run any test
      coverage metrics for you. It is left up to you to generate the raw code
      coverage data as part of your automated build process. Hudson, on the
      other hand, does an excellent job of <emphasis>reporting</emphasis> on
      the code coverage metrics, including keeping track of code coverage over
      time, and providing aggregate coverage across multiple application
      modules.</para>

      <para>Code coverage can be a complicated business, and it helps to
      understand the basic process that Cobertura follow, especially when you
      need to set it up in more low-level build scripting tools like Ant. Code
      coverage analysis works in three steps. First, it modifies (or
      "instruments") your application classes, so that they keep a tally of
      the number of times each line of code has been executed<footnote>
          <para>This is actually a slight over-simplification: in fact,
          Cobertura stores other data as well, such as how many times each
          possible outcome of a boolean test was executed. However this does
          not alter the general approach.</para>
        </footnote>. They store all this data in a special data file
      (Cobertura uses a file called
      <filename>cobertura.ser</filename>).</para>

      <para>When the application code has been instrumented, you run your
      tests against this instrumented code. At the end of the tests, Cobertura
      will have generated a data file containing the number of times each line
      of code was executed during the tests.</para>

      <para>Once this data file has been generated, Cobertura can use this
      data to generate a report in a more usable format, such as XML or
      HTML.</para>

      <sect3>
        <title>Integrating Cobertura with Maven</title>

        <para>Producing code coverage metrics with Cobertura in Maven is
        relatively straightforward. If all you are interested in is producing
        code coverage data, you just need to add the
        <command>cobertura-maven-plugin</command> to the build section of your
        <filename>pom.xml</filename> file:</para>

        <para><programlisting> &lt;project&gt;
   ...
   &lt;build&gt;
      &lt;plugins&gt;
         &lt;plugin&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
             &lt;artifactId&gt;cobertura-maven-plugin&lt;/artifactId&gt;
             &lt;version&gt;2.4&lt;/version&gt;
             &lt;configuration&gt;
             &lt;formats&gt;
                &lt;format&gt;html&lt;/format&gt;
                &lt;format&gt;xml&lt;/format&gt;
             &lt;/formats&gt;
           &lt;/configuration&gt;
         &lt;/plugin&gt;
         ...
      &lt;/plugins&gt;
   &lt;build&gt;
   ...
&lt;/project&gt;</programlisting>This will generate code coverage metrics when
        you invoke the Cobertura plugin directly:<screen>$ <command>mvn cobertura:cobertura</command></screen></para>

        <para>The code coverage data will be generated in the
        <filename>target/site/cobertura</filename> directory, in a file called
        <filename>coverage.xml</filename>. </para>

        <para>This approach, however, will instrument your classes and produce
        code coverage data for every build, which is inefficient. A better
        approach is to place this configuration in a special profile, as shown
        here:</para>

        <para><programlisting> &lt;project&gt;
   ...
   &lt;profiles&gt;
    &lt;profile&gt;
      &lt;id&gt;metrics&lt;/id&gt;
      &lt;build&gt;
        &lt;plugins&gt;
          &lt;plugin&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;cobertura-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.4&lt;/version&gt;
            &lt;configuration&gt;
              &lt;formats&gt;
                &lt;format&gt;html&lt;/format&gt;
                &lt;format&gt;xml&lt;/format&gt;
              &lt;/formats&gt;
            &lt;/configuration&gt;
          &lt;/plugin&gt;
        &lt;/plugins&gt;
      &lt;/build&gt;
    &lt;/profile&gt;
    ...
  &lt;/profiles&gt;
&lt;/project&gt;</programlisting>In this case, you would invoke the Cobertura
        plugin using the 'metrics' profile to generate the code coverage
        data:<screen>$ <command>mvn cobertura:cobertura -Pmetrics</command></screen></para>

        <para>Another approach is to include code coverage reporting in your
        Maven reports. This approach is considerably slower and more
        memory-hungry than just generating the coverage data, but it can make
        sense if you are also generating other code quality metrics and
        reports at the same time. If you want to do this, you need to also
        include the Maven Cobertura plugin in the reporting section, as shown
        here:</para>

        <para><programlisting> &lt;project&gt;
   ...
  &lt;reporting&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
        &lt;artifactId&gt;cobertura-maven-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.4&lt;/version&gt;
        &lt;configuration&gt;
          &lt;formats&gt;
            &lt;format&gt;html&lt;/format&gt;
            &lt;format&gt;xml&lt;/format&gt;
          &lt;/formats&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/reporting&gt;
&lt;/project&gt;</programlisting>Now the coverage data will be generated when
        you generate the Maven site for this project:<screen>$ <command>mvn site</command></screen></para>

        <para>At the time of writing, there is a limitation with the Maven
        Cobertura plugin - code coverage will be only recorded for tests
        executed during the <command>test</command> lifecycle phase, and not
        for tests executed during the <command>integration-test</command>
        phase.</para>
      </sect3>

      <sect3>
        <title>Integrating Cobertura with Ant</title>

        <para>Integrating Cobertura into your Ant build is more complicated
        than doing so in Maven. However it does give you a finer control over
        what classes are instrumented, and when coverage is measured.</para>

        <para>Cobertura comes bundled with an Ant task that you can use to
        integrate Cobertura into your Ant builds. You will need to download
        the latest Cobertura distribution, and unzip it somewhere on your hard
        disk. To make your build more portable, and therefore easier to deploy
        onto Hudson, it is a good idea to place the Cobertura distribution you
        are using within your project directory, and to save it in your
        version control system. This way it is easier to ensure that the build
        will use the same version of Cobertura no matter where it is
        run.</para>

        <para>Assuming you have downloaded the latest Cobertura installation
        and placed it within your project in a directory called
        <filename>tools</filename>, you could do something like
        this:<programlisting>&lt;property name="cobertura.dir" value="{basedir}/tools/cobertura" /&gt;<co
              id="co-ch04-cobertura-dir" />

&lt;path id="cobertura.classpath"&gt;<co id="co-ch04-cobertura-path" />
    &lt;fileset dir="${cobertura.dir}"&gt;
        &lt;include name="cobertura.jar" /&gt;<co id="co-ch04-cobertura-jar" />
        &lt;include name="lib/**/*.jar" /&gt;<co id="co-ch04-cobertura-libs" />
    &lt;/fileset&gt;
&lt;/path&gt;

&lt;taskdef classpathref="cobertura.classpath" resource="tasks.properties" /&gt;</programlisting></para>

        <para><calloutlist>
            <callout arearefs="co-ch04-cobertura-dir">
              <para>Tell Ant where your Cobertura installation is.</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-path">
              <para>We need to set up a classpath that Cobertura can use to
              run.</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-jar">
              <para>The path contains the Cobertura application itself;</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-libs">
              <para>And all of its dependencies.</para>
            </callout>
          </calloutlist>Next, you need to instrument your application classes.
        You have to be careful to place these instrumented classes in a
        separated directory, so that they don't get bundled up and deployed to
        production by accident.</para>

        <para><programlisting>&lt;target name="instrument" depends="init,compile"&gt;<co
              id="co-ch04-cobertura-instrumentation" />
    &lt;delete file="cobertura.ser"/&gt;<co id="co-ch04-cobertura-delete" />
    &lt;delete dir="${instrumented.dir}" /&gt;<co
              id="co-ch04-cobertura-delete-instrumented" />
    &lt;cobertura-instrument todir="${instrumented.dir}"&gt;<co
              id="co-ch04-cobertura-instrument" />
        &lt;fileset dir="${classes.dir}"&gt;
            &lt;include name="**/*.class" /&gt;
            &lt;exclude name="**/*Test.class" /&gt;
        &lt;/fileset&gt;
    &lt;/cobertura-instrument&gt;
&lt;/target&gt;</programlisting></para>

        <para><calloutlist>
            <callout arearefs="co-ch04-cobertura-instrumentation">
              <para>We can only instrument the application classes once they
              have been compiled.</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-delete">
              <para>Remove any coverage data generated by previous
              builds.</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-delete-instrumented">
              <para>Remove any previously instrumented classes.</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-instrument">
              <para>Instrument the application classes (but not the test
              classes) and place them in the ${instrumented.dir}
              directory.</para>
            </callout>
          </calloutlist></para>

        <para>At this stage, the ${instrumented.dir} directory contains an
        instrumented version of our application classes. Now all we need to do
        to generate some useful code coverage data is to run our unit tests
        against the classes in this directory:</para>

        <para><programlisting>&lt;target name="test-coverage" depends="instrument"&gt;
    &lt;junit fork="yes" dir="${basedir}"&gt;<co id="co-ch04-cobertura-junit" />
        &lt;classpath location="${instrumented.dir}" /&gt;
        &lt;classpath location="${classes.dir}" /&gt;
        &lt;classpath refid="cobertura.classpath" /&gt;<co
              id="co-ch04-cobertura-classpath" />

        &lt;formatter type="xml" /&gt;
        &lt;test name="${testcase}" todir="${reports.xml.dir}" if="testcase" /&gt;
        &lt;batchtest todir="${reports.xml.dir}" unless="testcase"&gt;
            &lt;fileset dir="${src.dir}"&gt;
                &lt;include name="**/*Test.java" /&gt;
            &lt;/fileset&gt;
        &lt;/batchtest&gt;
    &lt;/junit&gt;
&lt;/target&gt;</programlisting><calloutlist>
            <callout arearefs="co-ch04-cobertura-junit">
              <para>Run the JUnit tests against the instrumented application
              classes</para>
            </callout>

            <callout arearefs="co-ch04-cobertura-classpath">
              <para>The instrumented classes use Cobertura classes, so the
              Cobertura libraries also need to be on the classpath.</para>
            </callout>
          </calloutlist></para>

        <para>This will produce the raw test coverage data we need to produce
        the XML test coverage reports that Hudson can use. To actually produce
        these reports, we need to invoke another task, as shown here:</para>

        <para><programlisting>&lt;target name="coverage-report" depends="test-coverage"&gt;
    &lt;cobertura-report srcdir="${src.dir}" destdir="${coverage.xml.dir}" format="xml" /&gt;
&lt;/target&gt;</programlisting>Finally, don't forget to tidy up after your
        done: the <command>clean</command> target should delete not only the
        generated classes, but also the generated instrumented classes, the
        Cobertura coverage data, and the Cobertura reports:</para>

        <para><programlisting>&lt;target name="clean" description="Remove all files created by the build/test process."&gt;
    &lt;delete dir="${classes.dir}" /&gt;
    &lt;delete dir="${instrumented.dir}" /&gt;
    &lt;delete dir="${reports.dir}" /&gt;
    &lt;delete file="cobertura.log" /&gt;
    &lt;delete file="cobertura.ser" /&gt;
&lt;/target&gt;</programlisting>Once this is done, you are ready to integrate
        your coverage reports into Hudson.</para>
      </sect3>

      <sect3>
        <title>Installing the Cobertura code coverage plugin</title>

        <para>Once code coverage data is being generated as part of your build
        process, you can configure Hudson to report on it. This involves
        installing the Hudson Cobertura plugin. We went through this process
        in <xref linkend="sect-first-steps-metrics" />, but we'll run through
        it again to refresh your memory. Go to the 'Manage Hudson' screen, and
        click on 'Manage Plugins'. This will take you to the Plugin Manager
        screen. If Cobertura has not been installed, you will find the
        Cobertura Plugin in the 'Available' tab, in the 'Build Reports'
        section (see <xref linkend="fig-hudson-cobertura-plugin" />). To
        install it, just tick the checkbox and press enter (or scroll down to
        the bottom of the screen and click on the 'Install' button). Hudson
        will download and install the plugin for you. Once the downloading is
        done, you will need to restart your Hudson server.</para>

        <para><figure id="fig-hudson-cobertura-plugin">
            <title>Installing the Cobertura plugin</title>

            <mediaobject>
              <imageobject role="web">
                <imagedata align="center"
                           fileref="figs/web/hudson-cobertura-plugin.png"
                           width="4.3in" />
              </imageobject>
            </mediaobject>
          </figure></para>
      </sect3>

      <sect3>
        <title>Reporting on code coverage in your build</title>

        <para>Once you have installed the plugin, you can set up code coverage
        reporting in your build jobs. Since code coverage can be slow and
        memory-hungry, you would typically create a separate build job for
        this and other code quality metrics, to be run after the normal unit
        and integration tests. For very large projects, you may even want to
        set this up as a build that only runs on a nightly basis. Indeed,
        feedback on code coverage and other such metrics is usually not as
        time-critical as feedback on test results, and this will leave build
        executors free for build jobs that can benefit from snappy
        feedback.</para>

        <para>As we mentioned earlier, Hudson does not do any code coverage
        analysis itself - you need to configure your build to produce the
        Cobertura <filename>coverage.xml</filename> file before you can
        generate any nice graphs or reports, typically using one of the
        techniques we discussed previously (see</para>

        <para><xref linkend="fig-hudson-coverage-build-config" />). </para>

        <para><figure id="fig-hudson-coverage-build-config">
            <title>Your code coverage metrics build needs to generate the
            coverage data</title>

            <mediaobject>
              <imageobject role="web">
                <imagedata align="center"
                           fileref="figs/web/hudson-testing-cobertura-build.png"
                           width="4.3in" />
              </imageobject>
            </mediaobject>
          </figure>Once you have configured your build to produce some code
        coverage data, you can configure Cobertura in the 'Post-Action Builds'
        section of your build job. When you tick the 'Publish Cobertura
        Coverage Report' checkbox, you should see something like <xref
        linkend="fig-hudson-coverage-config" />.</para>

        <para><figure id="fig-hudson-coverage-config">
            <title>Configuring the test coverage metrics in Hudson</title>

            <mediaobject>
              <imageobject role="web">
                <imagedata align="center"
                           fileref="figs/web/hudson-config-cobertura.png"
                           width="4.3in" />
              </imageobject>
            </mediaobject>
          </figure></para>

        <para>The first and most important field here is the path to the
        Cobertura XML data that we generated. Your project may include a
        single <filename>coverage.xml</filename> file, or several (if your
        project is made up of sub-projects or Maven modules, for example). The
        path accepts Ant-style wildcards, so it is easy to include code
        coverage data from several files. For any Maven project, a path like
        '**/target/site/cobertura/coverage.xml' will include all of the code
        coverage metrics for all of the modules in the project.</para>

        <para>There are actually several types of code coverage, and it can
        sometimes be useful to distinguish between them. The most intuitive is
        Line Coverage, which counts the number of times any given line is
        executed during the automated tests. "Conditional Coverage" (also
        referred to as "Branch Coverage") takes into account whether the
        boolean expressions in <command>if</command> statements and the like
        are tested in a way that checks all the possible outcomes of the
        conditional expression. For example, consider the following code
        snippet:<programlisting>if (price &gt; 10000) {
  managerApprovalRequired = true;
}</programlisting></para>

        <para>To obtain full Conditional Coverage for this code, you would
        need to execute it twice: once with a value that is more than 10000,
        and one with a value of 10000 or less. </para>

        <para>Other more basic code coverage metrics include methods (how many
        methods in the application were exercised by the tests), classes and
        packages. </para>

        <para>Hudson lets you define which of these metrics you want to track.
        By default, the Cobertura plugin will record Conditional, Line, and
        Method coverage, which is usually plenty. However it is easy to add
        other coverage metrics if you think this might be useful for your
        team.</para>

        <para>Hudson code quality metrics are not simply a passive reporting
        process - Hudson lets you define how these metrics affect the build
        outcome. You can define threshold values for the coverage metrics that
        affect both the build outcome and the weather reports on the Hudson
        dashboard (see <xref
        linkend="fig-hudson-testing-coverage-stabiliy" />). Each coverage
        metric that you track takes three threshold values. </para>

        <para><figure id="fig-hudson-testing-coverage-stabiliy">
            <title>Configuring the test coverage metrics in Hudson</title>

            <mediaobject>
              <imageobject role="web">
                <imagedata align="center"
                           fileref="figs/web/hudson-testing-coverage-stabiliy.png"
                           width="4.3in" />
              </imageobject>
            </mediaobject>
          </figure>The first (the one with the sunny icon) is the minimum
        value necessary for the build to have a sunny weather icon. The second
        indicates the value below which the build will be attributed a stormy
        weather icon. Hudson will extrapolate between these values for the
        other more nuanced weather icons. </para>

        <para>The last threshold value is simply the value below which a build
        will be marked as 'unstable' - the yellow ball. While not quite as bad
        as the red ball (for a broken build), a yellow ball will still result
        in a notification message and will look bad on the dashboard.</para>

        <para>This feature is far from simply a cosmetic detail - it provides
        a valuable way of setting objective code quality goals for your
        projects. Although it cannot be interpreted along, falling code
        coverage is generally not a good sign in a project. So if you are
        serious about code coverage, use these threshold values to provide
        some hard feedback about when things are not up to scratch.</para>
      </sect3>

      <sect3>
        <title>Interpreting code coverage metrics</title>

        <para>Hudson displays your code coverage reports on the build job home
        page. The first time it runs, it produces a simple bar chart (see
        <xref linkend="fig-hudson-initial-coverage-report" />). From the
        second build onwards, a graph is shown, indicating the various types
        of coverage that you are tracking over time.</para>
      </sect3>

      <sect3>
        <title>Aggregating code coverage data</title>

        <para></para>
      </sect3>

      <sect3>
        <title>Running Cobertura on multi-module Maven projects</title>

        <para>Many Maven projects are organized into several modules.</para>

        <para>The Cobertura Hudson plugin is perfectly capable of aggregating
        code coverage metrics from the different modules in your project,
        which is convenient, since the Maven Cobertura plugin itself currently
        does not support this feature.</para>
      </sect3>
    </sect2>

    <sect2>
      <title>Measuring code coverage with Clover</title>

      <para></para>
    </sect2>
  </sect1>

  <sect1>
    <title>Conclusion</title>

    <para>TODO</para>
  </sect1>
</chapter>
